{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tf_nlp.data  import qa_qc\n",
    "from tf_nlp.utils import validation_split, find_common_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  4844\n",
      "Validate set size:  538\n",
      "Test  set size:  500\n",
      "Example piece of data (('What', 'is', 'the', 'main', 'language', 'of', 'Sao', 'Paulo', ',', 'Brazil', '?'), 'ENTY', 'lang')\n",
      "Intriguing properties: \n",
      "       a) quite a few duplicates detected:  70\n",
      "       b) Number of examples shared between train and test:  10\n",
      "          e.g:  QaQcDatum(question=('What', 'is', 'viscosity', '?'), main_cat='DESC', sub_cat='def')\n"
     ]
    }
   ],
   "source": [
    "train_and_validate, test = qa_qc(\"data/input4.txt\", \"data/test.txt\")\n",
    "\n",
    "print(\"Train set size: \", len(train))\n",
    "print(\"Validate set size: \", len(validate))\n",
    "print(\"Test  set size: \", len(test))\n",
    "print(\"Example piece of data\", train[0])\n",
    "print(\"Intriguing properties: \",)\n",
    "print(\"       a) quite a few duplicates detected: \", len(train_and_validate) - len(set(train_and_validate)), )\n",
    "common = list(find_common_examples(train_and_validate, test))\n",
    "print(\"       b) Number of examples shared between train and test: \", len(common))\n",
    "print(\"          e.g: \", common[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# deduplication and validation split\n",
    "train_and_validate = list(set(train_and_validate))\n",
    "train, validate = validation_split(train_and_validate, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Building the Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tf_nlp       import Vocab \n",
    "from tf_nlp.utils import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_questions(dataset):\n",
    "    return [example.question for example in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unknown words in train:  0.0\n",
      "Percentage of unknown words in validate:  11.273257935060196\n"
     ]
    }
   ],
   "source": [
    "naive_vocab = Vocab(extract_questions(train))\n",
    "print(\"Percentage of unknown words in train: \", 100.0 * naive_vocab.fraction_unknown(extract_questions(train)))\n",
    "print(\"Percentage of unknown words in validate: \", 100.0 * naive_vocab.fraction_unknown(extract_questions(validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unknown words in train:  1.785425428663863\n",
      "Percentage of unknown words in validate:  12.331265961327983\n"
     ]
    }
   ],
   "source": [
    "popular_words = Vocab.keep_n_most_frequent(flatten(extract_questions(train)), round(0.9 * len(naive_vocab)))\n",
    "vocab = Vocab(popular_words)\n",
    "print(\"Percentage of unknown words in train: \", 100.0 * vocab.fraction_unknown(extract_questions(train)))\n",
    "print(\"Percentage of unknown words in validate: \", 100.0 * vocab.fraction_unknown(extract_questions(validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops import functional_ops # for scan!\n",
    "from tf_nlp.models import Linear, GRU, Embedding\n",
    "from tf_nlp.utils  import get_pb, make_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "MAIN_CATEGORIES = list(set(ex.main_cat for ex in train_and_validate))\n",
    "SUB_CATEGORIES  = list(set(ex.sub_cat  for ex in train_and_validate))\n",
    "\n",
    "EMBEDDING_SIZE = 50\n",
    "HIDDEN_SIZE    = 100\n",
    "BATCH_SIZE     = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_examples(examples):\n",
    "    num_examples  = len(examples)\n",
    "    longest_question = max(len(example.question) for example in examples)\n",
    "    X     = np.empty((longest_question + 1, num_examples        ), dtype=np.int32)\n",
    "    Ymain = np.zeros((num_examples,         len(MAIN_CATEGORIES)), dtype=np.float32)\n",
    "    Ysub  = np.zeros((num_examples,         len(SUB_CATEGORIES) ), dtype=np.float32)\n",
    "    for i, example in enumerate(examples):\n",
    "        question, main_cat, sub_cat = example\n",
    "        X[:, i] = vocab.encode(example.question, pad_eos=longest_question + 1)\n",
    "        Ymain[i, MAIN_CATEGORIES.index(example.main_cat)] = 1.0\n",
    "        Ysub [i, SUB_CATEGORIES.index(sub_cat)]  = 1.0\n",
    "    return X, Ymain, Ysub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'session' in globals():\n",
    "    session.close()\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model componnents\n",
    "embedding       = Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "gru_cell        = GRU([EMBEDDING_SIZE,], HIDDEN_SIZE, final_nonlinearity=tf.nn.relu6)\n",
    "sentence_parser = SentenceParser(embedding, gru_cell)\n",
    "classifier      = Classifier(HIDDEN_SIZE, len(MAIN_CATEGORIES))\n",
    "\n",
    "# Define inputs\n",
    "input_idxes    = tf.placeholder(tf.int32,   shape=(None, None,),        name=\"input_idxes\")    # TIMESTEP  x BATCHSIZE\n",
    "output_onehots = tf.placeholder(tf.float32, shape=(None, NUM_MAIN_CAT), name=\"output_onehots\") # BATCHSIZE x NUM_CLASSES\n",
    "\n",
    "# execute the model\n",
    "sentence_hidden = sentence_parser.final_hidden(input_idxes)\n",
    "error           = classifier.error(sentence_hidden, output_onehots)\n",
    "num_correct     = classifier.num_correct(sentence_hidden, output_onehots)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op  = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(examples, batch_size, dataset_name=\"dataset\"):\n",
    "    acc_num_correct, acc_num_total = 0, 0\n",
    "    progress = get_pb(\"Accuracy on %s: \" % (dataset_name,))\n",
    "    batches = make_batches(examples, batch_size, sorting_key=lambda x:len(x.question))\n",
    "    \n",
    "    for batch in progress(batches):\n",
    "        X, Ymain, _ = batch_examples(batch)\n",
    "        batch_correct = session.run(num_correct, { input_idxes: X, output_onehots: Ymain})\n",
    "        acc_num_correct += batch_correct\n",
    "        acc_num_total   += len(batch)\n",
    "    return acc_num_correct / acc_num_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: Time: 0:00:30 |######################################################| 100%\n",
      "Accuracy on train: Time: 0:00:00 |############################################| 100%\n",
      "Accuracy on validate: Time: 0:00:00 |#########################################| 100%\n",
      "Epoch 1: Time: 0:00:31 |######################################################| 100%\n",
      "Accuracy on train: Time: 0:00:00 |############################################| 100%\n",
      "Accuracy on validate: ETA:  0:00:00 |####################                     |  50%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: accuracy on train: 99.1 %, validate: 84.8 %\n",
      "Epoch 1: accuracy on train: 99.6 %, validate: 86.1 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accuracy on validate: Time: 0:00:00 |#########################################| 100%\n",
      "Epoch 2: Time: 0:00:30 |######################################################| 100%\n",
      "Accuracy on train: Time: 0:00:00 |############################################| 100%\n",
      "Accuracy on validate: Time: 0:00:00 |#########################################| 100%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: accuracy on train: 99.9 %, validate: 85.5 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Time: 0:00:31 |######################################################| 100%\n",
      "Accuracy on train: Time: 0:00:00 |############################################| 100%\n",
      "Accuracy on validate: ETA:  0:00:00 |####################                     |  50%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: accuracy on train: 99.7 %, validate: 84.8 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accuracy on validate: Time: 0:00:00 |#########################################| 100%\n",
      "Epoch 4: ETA:  0:00:26 |########                                              |  15%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-219-3a555b7e9d23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0minput_idxes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_onehots\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mYmain\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0macc_train\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[1;36m100.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0macc_validate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"validate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 333\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    334\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 538\u001b[1;33m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 601\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    602\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    606\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 592\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    batches = make_batches(train, BATCH_SIZE, sorting_key=lambda x:len(x.question))\n",
    "    progress = get_pb(\"Epoch %d: \" % (epoch,))\n",
    "    for batch in progress(batches):\n",
    "        X, Ymain, _ = batch_examples(batch)\n",
    "        session.run(train_op, { input_idxes: X, output_onehots: Ymain})\n",
    "    acc_train    = 100.0 * accuracy(train,    100, \"train\")\n",
    "    acc_validate = 100.0 * accuracy(validate, 100, \"validate\")\n",
    "    print(\"Epoch %d: accuracy on train: %.1f %%, validate: %.1f %%\" % (epoch, acc_train, acc_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
